{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHz7xotoUh_o",
        "outputId": "fb96e972-a95a-4923-f7fc-57de6da6c0e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-09 23:53:14--  https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.12, 3.165.160.59, 3.165.160.61, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/74/40/7440e009c158a833d66fbb43b7d5507adb4a06a37827658c968b4073fcec5ec4/80f532552e3d56e366226f428395de8285a671f2da1d5fd68563741181b77a95?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf%3B+filename%3D%22Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf%22%3B&Expires=1741567994&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MTU2Nzk5NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzc0LzQwLzc0NDBlMDA5YzE1OGE4MzNkNjZmYmI0M2I3ZDU1MDdhZGI0YTA2YTM3ODI3NjU4Yzk2OGI0MDczZmNlYzVlYzQvODBmNTMyNTUyZTNkNTZlMzY2MjI2ZjQyODM5NWRlODI4NWE2NzFmMmRhMWQ1ZmQ2ODU2Mzc0MTE4MWI3N2E5NT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=OtnsquDzkkdJil4MjidLjoy8h7eLGIjR3B37DNYFNdY75mH79xLwdMzqJM-WLU%7EUlhVVi5W3VYQMddffR0sdU23%7EnXeBda3r5akXbNR4896ACEVXZrIcFJ%7EBE1nPHZ1-r5iiOzjPubAwHdauEN0%7ErdXa25DRSxcTc4IpCFlXxHemtHdQIPGMTvgCRyscswo53TU4VF9Vp%7ER4bJqScmjrpFHPTS%7EzQMJrC2BduFEvG2oI84LV7rq8LC1y9faPK0COhRRfS%7ERTQZ5sq6DAaulvea1UWGTSyLgpisISD931ORHh6OmEIqARJYj2g92ZV1rv0aJHAVBf4qpVEA8XkPVZ4A__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-03-09 23:53:14--  https://cdn-lfs-us-1.hf.co/repos/74/40/7440e009c158a833d66fbb43b7d5507adb4a06a37827658c968b4073fcec5ec4/80f532552e3d56e366226f428395de8285a671f2da1d5fd68563741181b77a95?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf%3B+filename%3D%22Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf%22%3B&Expires=1741567994&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0MTU2Nzk5NH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzc0LzQwLzc0NDBlMDA5YzE1OGE4MzNkNjZmYmI0M2I3ZDU1MDdhZGI0YTA2YTM3ODI3NjU4Yzk2OGI0MDczZmNlYzVlYzQvODBmNTMyNTUyZTNkNTZlMzY2MjI2ZjQyODM5NWRlODI4NWE2NzFmMmRhMWQ1ZmQ2ODU2Mzc0MTE4MWI3N2E5NT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=OtnsquDzkkdJil4MjidLjoy8h7eLGIjR3B37DNYFNdY75mH79xLwdMzqJM-WLU%7EUlhVVi5W3VYQMddffR0sdU23%7EnXeBda3r5akXbNR4896ACEVXZrIcFJ%7EBE1nPHZ1-r5iiOzjPubAwHdauEN0%7ErdXa25DRSxcTc4IpCFlXxHemtHdQIPGMTvgCRyscswo53TU4VF9Vp%7ER4bJqScmjrpFHPTS%7EzQMJrC2BduFEvG2oI84LV7rq8LC1y9faPK0COhRRfS%7ERTQZ5sq6DAaulvea1UWGTSyLgpisISD931ORHh6OmEIqARJYj2g92ZV1rv0aJHAVBf4qpVEA8XkPVZ4A__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.238.238.106, 18.238.238.6, 18.238.238.75, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.238.238.106|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2241003904 (2.1G) [binary/octet-stream]\n",
            "Saving to: ‘Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf’\n",
            "\n",
            "Llama-3.2-3B-Instru 100%[===================>]   2.09G  21.2MB/s    in 59s     \n",
            "\n",
            "2025-03-09 23:54:13 (36.5 MB/s) - ‘Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf’ saved [2241003904/2241003904]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/bartowski/Llama-3.2-3B-Instruct-uncensored-GGUF/resolve/main/Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ggml-org/llama.cpp/releases/download/b3972/llama-b3972-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98XVZqNxU5r4",
        "outputId": "c0704057-d79f-4922-c0cf-ecee5a8f04f9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-09 23:54:59--  https://github.com/ggml-org/llama.cpp/releases/download/b3972/llama-b3972-bin-ubuntu-x64.zip\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/612354784/84bc4565-a115-4672-9634-b17aaa837cfd?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250309%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250309T235500Z&X-Amz-Expires=300&X-Amz-Signature=aee30d0ecdfc189dfaea1f5aca0ad3474a80e963e49e7f68889f549c3be1dc01&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllama-b3972-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-09 23:55:00--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/612354784/84bc4565-a115-4672-9634-b17aaa837cfd?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250309%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250309T235500Z&X-Amz-Expires=300&X-Amz-Signature=aee30d0ecdfc189dfaea1f5aca0ad3474a80e963e49e7f68889f549c3be1dc01&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllama-b3972-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 61602731 (59M) [application/octet-stream]\n",
            "Saving to: ‘llama-b3972-bin-ubuntu-x64.zip’\n",
            "\n",
            "llama-b3972-bin-ubu 100%[===================>]  58.75M  44.1MB/s    in 1.3s    \n",
            "\n",
            "2025-03-09 23:55:01 (44.1 MB/s) - ‘llama-b3972-bin-ubuntu-x64.zip’ saved [61602731/61602731]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip llama-b3972-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_oR2nCKVVs4",
        "outputId": "6a82356d-d7de-4591-db04-85dbedb3ad83"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  llama-b3972-bin-ubuntu-x64.zip\n",
            "  inflating: build/bin/LICENSE       \n",
            "  inflating: build/bin/llama-baby-llama  \n",
            "  inflating: build/bin/llama-batched  \n",
            "  inflating: build/bin/llama-batched-bench  \n",
            "  inflating: build/bin/llama-bench   \n",
            "  inflating: build/bin/llama-cli     \n",
            "  inflating: build/bin/llama-convert-llama2c-to-ggml  \n",
            "  inflating: build/bin/llama-cvector-generator  \n",
            "  inflating: build/bin/llama-embedding  \n",
            "  inflating: build/bin/llama-eval-callback  \n",
            "  inflating: build/bin/llama-export-lora  \n",
            "  inflating: build/bin/llama-gbnf-validator  \n",
            "  inflating: build/bin/llama-gguf    \n",
            "  inflating: build/bin/llama-gguf-hash  \n",
            "  inflating: build/bin/llama-gguf-split  \n",
            "  inflating: build/bin/llama-gritlm  \n",
            "  inflating: build/bin/llama-imatrix  \n",
            "  inflating: build/bin/llama-infill  \n",
            "  inflating: build/bin/llama-llava-cli  \n",
            "  inflating: build/bin/llama-lookahead  \n",
            "  inflating: build/bin/llama-lookup  \n",
            "  inflating: build/bin/llama-lookup-create  \n",
            "  inflating: build/bin/llama-lookup-merge  \n",
            "  inflating: build/bin/llama-lookup-stats  \n",
            "  inflating: build/bin/llama-minicpmv-cli  \n",
            "  inflating: build/bin/llama-parallel  \n",
            "  inflating: build/bin/llama-passkey  \n",
            "  inflating: build/bin/llama-perplexity  \n",
            "  inflating: build/bin/llama-q8dot   \n",
            "  inflating: build/bin/llama-quantize  \n",
            "  inflating: build/bin/llama-quantize-stats  \n",
            "  inflating: build/bin/llama-retrieval  \n",
            "  inflating: build/bin/llama-save-load-state  \n",
            "  inflating: build/bin/llama-server  \n",
            "  inflating: build/bin/llama-simple  \n",
            "  inflating: build/bin/llama-speculative  \n",
            "  inflating: build/bin/llama-tokenize  \n",
            "  inflating: build/bin/llama-vdot    \n",
            "  inflating: build/bin/rpc-server    \n",
            "  inflating: build/bin/test-arg-parser  \n",
            "  inflating: build/bin/test-autorelease  \n",
            "  inflating: build/bin/test-backend-ops  \n",
            "  inflating: build/bin/test-barrier  \n",
            "  inflating: build/bin/test-c        \n",
            "  inflating: build/bin/test-chat-template  \n",
            "  inflating: build/bin/test-grad0    \n",
            "  inflating: build/bin/test-grammar-integration  \n",
            "  inflating: build/bin/test-grammar-parser  \n",
            "  inflating: build/bin/test-json-schema-to-grammar  \n",
            "  inflating: build/bin/test-llama-grammar  \n",
            "  inflating: build/bin/test-log      \n",
            "  inflating: build/bin/test-model-load-cancel  \n",
            "  inflating: build/bin/test-quantize-fns  \n",
            "  inflating: build/bin/test-quantize-perf  \n",
            "  inflating: build/bin/test-rope     \n",
            "  inflating: build/bin/test-sampling  \n",
            "  inflating: build/bin/test-tokenizer-0  \n",
            "  inflating: build/bin/test-tokenizer-1-bpe  \n",
            "  inflating: build/bin/test-tokenizer-1-spm  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVf1pJc_Va6P",
        "outputId": "dbb909fb-99b4-4f8a-f2f5-4e3b93a60832"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- common params -----\n",
            "\n",
            "-h,    --help, --usage                  print usage and exit\n",
            "--version                               show version and build info\n",
            "--verbose-prompt                        print a verbose prompt before generation (default: false)\n",
            "-t,    --threads N                      number of threads to use during generation (default: -1)\n",
            "                                        (env: LLAMA_ARG_THREADS)\n",
            "-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:\n",
            "                                        same as --threads)\n",
            "-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range\n",
            "                                        (default: \"\")\n",
            "-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask\n",
            "--cpu-strict <0|1>                      use strict CPU placement (default: 0)\n",
            "--prio N                                set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)\n",
            "-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch\n",
            "                                        (default: same as --cpu-mask)\n",
            "-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch\n",
            "--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)\n",
            "--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)\n",
            "-c,    --ctx-size N                     size of the prompt context (default: 0, 0 = loaded from model)\n",
            "                                        (env: LLAMA_ARG_CTX_SIZE)\n",
            "-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity, -2 = until\n",
            "                                        context filled)\n",
            "                                        (env: LLAMA_ARG_N_PREDICT)\n",
            "-b,    --batch-size N                   logical maximum batch size (default: 2048)\n",
            "                                        (env: LLAMA_ARG_BATCH)\n",
            "-ub,   --ubatch-size N                  physical maximum batch size (default: 512)\n",
            "                                        (env: LLAMA_ARG_UBATCH)\n",
            "--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =\n",
            "                                        all)\n",
            "-fa,   --flash-attn                     enable Flash Attention (default: disabled)\n",
            "                                        (env: LLAMA_ARG_FLASH_ATTN)\n",
            "-p,    --prompt PROMPT                  prompt to start generation with\n",
            "                                        if -cnv is set, this will be used as system prompt\n",
            "--no-perf                               disable internal libllama performance timings (default: false)\n",
            "                                        (env: LLAMA_ARG_NO_PERF)\n",
            "-f,    --file FNAME                     a file containing the prompt (default: none)\n",
            "-bf,   --binary-file FNAME              binary file containing the prompt (default: none)\n",
            "-e,    --escape                         process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\) (default: true)\n",
            "--no-escape                             do not process escape sequences\n",
            "--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by\n",
            "                                        the model\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)\n",
            "--rope-scale N                          RoPE context scaling factor, expands context by a factor of N\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALE)\n",
            "--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from\n",
            "                                        model)\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)\n",
            "--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)\n",
            "--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training\n",
            "                                        context size)\n",
            "                                        (env: LLAMA_ARG_YARN_ORIG_CTX)\n",
            "--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full\n",
            "                                        interpolation)\n",
            "                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)\n",
            "--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)\n",
            "--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_SLOW)\n",
            "--yarn-beta-fast N                      YaRN: low correction dim or beta (default: 32.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_FAST)\n",
            "-dkvc, --dump-kv-cache                  verbose print of the KV cache\n",
            "-nkvo, --no-kv-offload                  disable KV offload\n",
            "                                        (env: LLAMA_ARG_NO_KV_OFFLOAD)\n",
            "-ctk,  --cache-type-k TYPE              KV cache data type for K (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K)\n",
            "-ctv,  --cache-type-v TYPE              KV cache data type for V (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V)\n",
            "-dt,   --defrag-thold N                 KV cache defragmentation threshold (default: -1.0, < 0 - disabled)\n",
            "                                        (env: LLAMA_ARG_DEFRAG_THOLD)\n",
            "-np,   --parallel N                     number of parallel sequences to decode (default: 1)\n",
            "                                        (env: LLAMA_ARG_N_PARALLEL)\n",
            "--rpc SERVERS                           comma separated list of RPC servers\n",
            "                                        (env: LLAMA_ARG_RPC)\n",
            "--mlock                                 force system to keep model in RAM rather than swapping or compressing\n",
            "                                        (env: LLAMA_ARG_MLOCK)\n",
            "--no-mmap                               do not memory-map model (slower load but may reduce pageouts if not\n",
            "                                        using mlock)\n",
            "                                        (env: LLAMA_ARG_NO_MMAP)\n",
            "--numa TYPE                             attempt optimizations that help on some NUMA systems\n",
            "                                        - distribute: spread execution evenly over all nodes\n",
            "                                        - isolate: only spawn threads on CPUs on the node that execution\n",
            "                                        started on\n",
            "                                        - numactl: use the CPU map provided by numactl\n",
            "                                        if run without this previously, it is recommended to drop the system\n",
            "                                        page cache before using this\n",
            "                                        see https://github.com/ggerganov/llama.cpp/issues/1437\n",
            "                                        (env: LLAMA_ARG_NUMA)\n",
            "-ngl,  --gpu-layers, --n-gpu-layers N   number of layers to store in VRAM\n",
            "                                        (env: LLAMA_ARG_N_GPU_LAYERS)\n",
            "-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\n",
            "                                        - none: use one GPU only\n",
            "                                        - layer (default): split layers and KV across GPUs\n",
            "                                        - row: split rows across GPUs\n",
            "                                        (env: LLAMA_ARG_SPLIT_MODE)\n",
            "-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of\n",
            "                                        proportions, e.g. 3,1\n",
            "                                        (env: LLAMA_ARG_TENSOR_SPLIT)\n",
            "-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for\n",
            "                                        intermediate results and KV (with split-mode = row) (default: 0)\n",
            "                                        (env: LLAMA_ARG_MAIN_GPU)\n",
            "--check-tensors                         check model tensor data for invalid values (default: false)\n",
            "--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified\n",
            "                                        multiple times.\n",
            "                                        types: int, float, bool, str. example: --override-kv\n",
            "                                        tokenizer.ggml.add_bos_token=bool:false\n",
            "--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)\n",
            "--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use\n",
            "                                        multiple adapters)\n",
            "--control-vector FNAME                  add a control vector\n",
            "                                        note: this argument can be repeated to add multiple control vectors\n",
            "--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE\n",
            "                                        note: this argument can be repeated to add multiple scaled control\n",
            "                                        vectors\n",
            "--control-vector-layer-range START END\n",
            "                                        layer range to apply the control vector(s) to, start and end inclusive\n",
            "-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`\n",
            "                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)\n",
            "                                        (env: LLAMA_ARG_MODEL)\n",
            "-mu,   --model-url MODEL_URL            model download url (default: unused)\n",
            "                                        (env: LLAMA_ARG_MODEL_URL)\n",
            "-hfr,  --hf-repo REPO                   Hugging Face model repository (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO)\n",
            "-hff,  --hf-file FILE                   Hugging Face model file (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE)\n",
            "-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment\n",
            "                                        variable)\n",
            "                                        (env: HF_TOKEN)\n",
            "-ld,   --logdir LOGDIR                  path under which to save YAML logs (no logging if unset)\n",
            "--log-disable                           Log disable\n",
            "--log-file FNAME                        Log to file\n",
            "--log-colors                            Enable colored logging\n",
            "                                        (env: LLAMA_LOG_COLORS)\n",
            "-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for\n",
            "                                        debugging)\n",
            "-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be\n",
            "                                        ignored.\n",
            "                                        (env: LLAMA_LOG_VERBOSITY)\n",
            "--log-prefix                            Enable prefx in log messages\n",
            "                                        (env: LLAMA_LOG_PREFIX)\n",
            "--log-timestamps                        Enable timestamps in log messages\n",
            "                                        (env: LLAMA_LOG_TIMESTAMPS)\n",
            "\n",
            "\n",
            "----- sampling params -----\n",
            "\n",
            "--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by\n",
            "                                        ';'\n",
            "                                        (default: top_k;tfs_z;typ_p;top_p;min_p;xtc;temperature)\n",
            "-s,    --seed SEED                      RNG seed (default: -1, use random seed for -1)\n",
            "--sampling-seq SEQUENCE                 simplified sequence for samplers that will be used (default: kfypmxt)\n",
            "--ignore-eos                            ignore end of stream token and continue generating (implies\n",
            "                                        --logit-bias EOS-inf)\n",
            "--penalize-nl                           penalize newline tokens (default: false)\n",
            "--temp N                                temperature (default: 0.8)\n",
            "--top-k N                               top-k sampling (default: 40, 0 = disabled)\n",
            "--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)\n",
            "--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)\n",
            "--tfs N                                 tail free sampling, parameter z (default: 1.0, 1.0 = disabled)\n",
            "--xtc-probability N                     xtc probability (default: 0.0, 0.0 = disabled)\n",
            "--xtc-threshold N                       xtc threshold (default: 0.1, 1.0 = disabled)\n",
            "--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
            "--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1\n",
            "                                        = ctx_size)\n",
            "--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
            "--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
            "--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
            "--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
            "--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)\n",
            "--mirostat N                            use Mirostat sampling.\n",
            "                                        Top K, Nucleus, Tail Free and Locally Typical samplers are ignored if\n",
            "                                        used.\n",
            "                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
            "--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)\n",
            "--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)\n",
            "-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,\n",
            "                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
            "                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
            "--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/\n",
            "                                        dir) (default: '')\n",
            "--grammar-file FNAME                    file to read grammar from\n",
            "-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.\n",
            "                                        `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "\n",
            "\n",
            "----- example-specific params -----\n",
            "\n",
            "--no-display-prompt                     don't print prompt at generation (default: false)\n",
            "-co,   --color                          colorise output to distinguish prompt and user input from generations\n",
            "                                        (default: false)\n",
            "--no-context-shift                      disables context shift on inifinite text generation (default:\n",
            "                                        disabled)\n",
            "                                        (env: LLAMA_ARG_NO_CONTEXT_SHIFT)\n",
            "-ptc,  --print-token-count N            print token count every N tokens (default: -1)\n",
            "--prompt-cache FNAME                    file to cache prompt state for faster startup (default: none)\n",
            "--prompt-cache-all                      if specified, saves user input and generations to cache as well\n",
            "--prompt-cache-ro                       if specified, uses the prompt cache but does not update it\n",
            "-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode\n",
            "-sp,   --special                        special tokens output enabled (default: false)\n",
            "-cnv,  --conversation                   run in conversation mode:\n",
            "                                        - does not print special tokens and suffix/prefix\n",
            "                                        - interactive mode is also enabled\n",
            "                                        (default: false)\n",
            "-i,    --interactive                    run in interactive mode (default: false)\n",
            "-if,   --interactive-first              run in interactive mode and wait for input right away (default: false)\n",
            "-mli,  --multiline-input                allows you to write or paste multiple lines without ending each in '\\'\n",
            "--in-prefix-bos                         prefix BOS to user inputs, preceding the `--in-prefix` string\n",
            "--in-prefix STRING                      string to prefix user inputs with (default: empty)\n",
            "--in-suffix STRING                      string to suffix after user inputs with (default: empty)\n",
            "--no-warmup                             skip warming up the model with an empty run\n",
            "-gan,  --grp-attn-n N                   group-attention factor (default: 1)\n",
            "                                        (env: LLAMA_ARG_GRP_ATTN_N)\n",
            "-gaw,  --grp-attn-w N                   group-attention width (default: 512)\n",
            "                                        (env: LLAMA_ARG_GRP_ATTN_W)\n",
            "--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's\n",
            "                                        metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted:\n",
            "                                        https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE)\n",
            "--simple-io                             use basic IO for better compatibility in subprocesses and limited\n",
            "                                        consoles\n",
            "\n",
            "example usage:\n",
            "\n",
            "  text generation:     build/bin/llama-cli -m your_model.gguf -p \"I believe the meaning of life is\" -n 128\n",
            "\n",
            "  chat (conversation): build/bin/llama-cli -m your_model.gguf -p \"You are a helpful assistant\" -cnv\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " --color --temp 0.1 -n 10"
      ],
      "metadata": {
        "id": "VT22djcTWz8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -m /content/Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf -p \"hi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNCcyG6kVYv1",
        "outputId": "da053921-83cf-4118-9151-33e8c336bf2c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 3972 (167a5156) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 256 tensors from /content/Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Meta Llama\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Llama-3.2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Llama-3.2-3B-Instruct-unc...\n",
            "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 196\n",
            "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type q4_K:  169 tensors\n",
            "llama_model_loader: - type q6_K:   29 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_layer          = 28\n",
            "llm_load_print_meta: n_head           = 24\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 3\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 3.61 B\n",
            "llm_load_print_meta: model size       = 2.08 GiB (4.95 BPW) \n",
            "llm_load_print_meta: general.name     = Llama 3.2 3B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: ggml ctx size =    0.12 MiB\n",
            "llm_load_tensors: offloading 0 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 0/29 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  2129.71 MiB\n",
            "..............................................................................\n",
            "llama_new_context_with_model: n_ctx      = 131072\n",
            "llama_new_context_with_model: n_batch    = 2048\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/build/bin/llama-cli -m /content/Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ojZKVH34W991",
        "outputId": "42d5794f-fc1e-45f5-8d09-2d0b511917bf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 3972 (167a5156) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 256 tensors from /content/Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Meta Llama\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Llama-3.2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Llama-3.2-3B-Instruct-unc...\n",
            "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 196\n",
            "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type q4_K:  169 tensors\n",
            "llama_model_loader: - type q6_K:   29 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_layer          = 28\n",
            "llm_load_print_meta: n_head           = 24\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 3\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 3.61 B\n",
            "llm_load_print_meta: model size       = 2.08 GiB (4.95 BPW) \n",
            "llm_load_print_meta: general.name     = Llama 3.2 3B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: ggml ctx size =    0.12 MiB\n",
            "llm_load_tensors: offloading 0 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 0/29 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =  2129.71 MiB\n",
            "..............................................................................\n",
            "llama_new_context_with_model: n_ctx      = 131072\n",
            "llama_new_context_with_model: n_batch    = 2048\n",
            "llama_new_context_with_model: n_ubatch   = 512\n",
            "llama_new_context_with_model: flash_attn = 0\n",
            "llama_new_context_with_model: freq_base  = 500000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ggml-org/llama.cpp/releases/download/b4856/llama-b4856-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA54KtqNXVXl",
        "outputId": "e1069325-9d4b-4ad4-9106-429a9e3791b3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-10 00:03:50--  https://github.com/ggml-org/llama.cpp/releases/download/b4856/llama-b4856-bin-ubuntu-x64.zip\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/612354784/f3ab1a27-1604-43d4-9da7-48ed2a9561f6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250310%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250310T000350Z&X-Amz-Expires=300&X-Amz-Signature=44b65c84d8b7736400d7d036a9a6a5a855b6a1386af120ffac13a04561a88954&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllama-b4856-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-10 00:03:50--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/612354784/f3ab1a27-1604-43d4-9da7-48ed2a9561f6?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250310%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250310T000350Z&X-Amz-Expires=300&X-Amz-Signature=44b65c84d8b7736400d7d036a9a6a5a855b6a1386af120ffac13a04561a88954&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllama-b4856-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28943174 (28M) [application/octet-stream]\n",
            "Saving to: ‘llama-b4856-bin-ubuntu-x64.zip’\n",
            "\n",
            "llama-b4856-bin-ubu 100%[===================>]  27.60M   105MB/s    in 0.3s    \n",
            "\n",
            "2025-03-10 00:03:51 (105 MB/s) - ‘llama-b4856-bin-ubuntu-x64.zip’ saved [28943174/28943174]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip llama-b4856-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gj6hm-LIXcKN",
        "outputId": "249831ef-5f3d-4ca8-dba4-fc89a5d1ac44"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  llama-b4856-bin-ubuntu-x64.zip\n",
            "  inflating: build/bin/LICENSE       \n",
            "  inflating: build/bin/LICENSE.linenoise.cpp  \n",
            "  inflating: build/bin/libggml-base.so  \n",
            "  inflating: build/bin/libggml-cpu.so  \n",
            "  inflating: build/bin/libggml-rpc.so  \n",
            "  inflating: build/bin/libggml.so    \n",
            "  inflating: build/bin/libllama.so   \n",
            "  inflating: build/bin/libllava_shared.so  \n",
            "  inflating: build/bin/llama-batched  \n",
            "  inflating: build/bin/llama-batched-bench  \n",
            "  inflating: build/bin/llama-bench   \n",
            "  inflating: build/bin/llama-cli     \n",
            "  inflating: build/bin/llama-convert-llama2c-to-ggml  \n",
            "  inflating: build/bin/llama-cvector-generator  \n",
            "  inflating: build/bin/llama-embedding  \n",
            "  inflating: build/bin/llama-eval-callback  \n",
            "  inflating: build/bin/llama-export-lora  \n",
            "  inflating: build/bin/llama-gbnf-validator  \n",
            "  inflating: build/bin/llama-gen-docs  \n",
            "  inflating: build/bin/llama-gguf    \n",
            "  inflating: build/bin/llama-gguf-hash  \n",
            "  inflating: build/bin/llama-gguf-split  \n",
            "  inflating: build/bin/llama-gritlm  \n",
            "  inflating: build/bin/llama-imatrix  \n",
            "  inflating: build/bin/llama-infill  \n",
            "  inflating: build/bin/llama-llava-cli  \n",
            "  inflating: build/bin/llama-llava-clip-quantize-cli  \n",
            "  inflating: build/bin/llama-lookahead  \n",
            "  inflating: build/bin/llama-lookup  \n",
            "  inflating: build/bin/llama-lookup-create  \n",
            "  inflating: build/bin/llama-lookup-merge  \n",
            "  inflating: build/bin/llama-lookup-stats  \n",
            "  inflating: build/bin/llama-minicpmv-cli  \n",
            "  inflating: build/bin/llama-parallel  \n",
            "  inflating: build/bin/llama-passkey  \n",
            "  inflating: build/bin/llama-perplexity  \n",
            "  inflating: build/bin/llama-q8dot   \n",
            "  inflating: build/bin/llama-quantize  \n",
            "  inflating: build/bin/llama-quantize-stats  \n",
            "  inflating: build/bin/llama-qwen2vl-cli  \n",
            "  inflating: build/bin/llama-retrieval  \n",
            "  inflating: build/bin/llama-run     \n",
            "  inflating: build/bin/llama-save-load-state  \n",
            "  inflating: build/bin/llama-server  \n",
            "  inflating: build/bin/llama-simple  \n",
            "  inflating: build/bin/llama-simple-chat  \n",
            "  inflating: build/bin/llama-speculative  \n",
            "  inflating: build/bin/llama-speculative-simple  \n",
            "  inflating: build/bin/llama-tokenize  \n",
            "  inflating: build/bin/llama-tts     \n",
            "  inflating: build/bin/llama-vdot    \n",
            "  inflating: build/bin/rpc-server    \n",
            "  inflating: build/bin/test-arg-parser  \n",
            "  inflating: build/bin/test-autorelease  \n",
            "  inflating: build/bin/test-backend-ops  \n",
            "  inflating: build/bin/test-barrier  \n",
            "  inflating: build/bin/test-c        \n",
            "  inflating: build/bin/test-chat     \n",
            "  inflating: build/bin/test-chat-template  \n",
            "  inflating: build/bin/test-gguf     \n",
            "  inflating: build/bin/test-grammar-integration  \n",
            "  inflating: build/bin/test-grammar-parser  \n",
            "  inflating: build/bin/test-json-schema-to-grammar  \n",
            "  inflating: build/bin/test-llama-grammar  \n",
            "  inflating: build/bin/test-log      \n",
            "  inflating: build/bin/test-model-load-cancel  \n",
            "  inflating: build/bin/test-quantize-fns  \n",
            "  inflating: build/bin/test-quantize-perf  \n",
            "  inflating: build/bin/test-rope     \n",
            "  inflating: build/bin/test-sampling  \n",
            "  inflating: build/bin/test-tokenizer-0  \n",
            "  inflating: build/bin/test-tokenizer-1-bpe  \n",
            "  inflating: build/bin/test-tokenizer-1-spm  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/build/bin/llama-cli -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLZxmF9aXi_l",
        "outputId": "4deb90fe-6808-4383-f429-d4ed9ba5f7e9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/build/bin/llama-cli: error while loading shared libraries: libllama.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ggml-org/llama.cpp/releases/download/b4768/llama-b4768-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WxIDq6ZKXkcd",
        "outputId": "995f97ad-73d7-470a-d1e7-f12300b6878c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-10 00:05:43--  https://github.com/ggml-org/llama.cpp/releases/download/b4768/llama-b4768-bin-ubuntu-x64.zip\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/612354784/b62d49e8-d629-48e0-951f-a3f65c8d9e15?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250310%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250310T000544Z&X-Amz-Expires=300&X-Amz-Signature=b57ba178bc1b0351ede2d563319599469d46b68fc51fe4d3cab89dbe050ef11f&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllama-b4768-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-10 00:05:44--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/612354784/b62d49e8-d629-48e0-951f-a3f65c8d9e15?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250310%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250310T000544Z&X-Amz-Expires=300&X-Amz-Signature=b57ba178bc1b0351ede2d563319599469d46b68fc51fe4d3cab89dbe050ef11f&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllama-b4768-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 28179532 (27M) [application/octet-stream]\n",
            "Saving to: ‘llama-b4768-bin-ubuntu-x64.zip’\n",
            "\n",
            "llama-b4768-bin-ubu 100%[===================>]  26.87M  40.7MB/s    in 0.7s    \n",
            "\n",
            "2025-03-10 00:05:45 (40.7 MB/s) - ‘llama-b4768-bin-ubuntu-x64.zip’ saved [28179532/28179532]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip llama-b4768-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFXtKogWXy7l",
        "outputId": "05e78718-9309-4ecc-a7b5-06456679c0f3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  llama-b4768-bin-ubuntu-x64.zip\n",
            "  inflating: build/bin/LICENSE       \n",
            "  inflating: build/bin/LICENSE.linenoise.cpp  \n",
            "  inflating: build/bin/libggml-base.so  \n",
            "  inflating: build/bin/libggml-cpu.so  \n",
            "  inflating: build/bin/libggml-rpc.so  \n",
            "  inflating: build/bin/libggml.so    \n",
            "  inflating: build/bin/libllama.so   \n",
            "  inflating: build/bin/libllava_shared.so  \n",
            "  inflating: build/bin/llama-batched  \n",
            "  inflating: build/bin/llama-batched-bench  \n",
            "  inflating: build/bin/llama-bench   \n",
            "  inflating: build/bin/llama-cli     \n",
            "  inflating: build/bin/llama-convert-llama2c-to-ggml  \n",
            "  inflating: build/bin/llama-cvector-generator  \n",
            "  inflating: build/bin/llama-embedding  \n",
            "  inflating: build/bin/llama-eval-callback  \n",
            "  inflating: build/bin/llama-export-lora  \n",
            "  inflating: build/bin/llama-gbnf-validator  \n",
            "  inflating: build/bin/llama-gen-docs  \n",
            "  inflating: build/bin/llama-gguf    \n",
            "  inflating: build/bin/llama-gguf-hash  \n",
            "  inflating: build/bin/llama-gguf-split  \n",
            "  inflating: build/bin/llama-gritlm  \n",
            "  inflating: build/bin/llama-imatrix  \n",
            "  inflating: build/bin/llama-infill  \n",
            "  inflating: build/bin/llama-llava-cli  \n",
            "  inflating: build/bin/llama-llava-clip-quantize-cli  \n",
            "  inflating: build/bin/llama-lookahead  \n",
            "  inflating: build/bin/llama-lookup  \n",
            "  inflating: build/bin/llama-lookup-create  \n",
            "  inflating: build/bin/llama-lookup-merge  \n",
            "  inflating: build/bin/llama-lookup-stats  \n",
            "  inflating: build/bin/llama-minicpmv-cli  \n",
            "  inflating: build/bin/llama-parallel  \n",
            "  inflating: build/bin/llama-passkey  \n",
            "  inflating: build/bin/llama-perplexity  \n",
            "  inflating: build/bin/llama-q8dot   \n",
            "  inflating: build/bin/llama-quantize  \n",
            "  inflating: build/bin/llama-quantize-stats  \n",
            "  inflating: build/bin/llama-qwen2vl-cli  \n",
            "  inflating: build/bin/llama-retrieval  \n",
            "  inflating: build/bin/llama-run     \n",
            "  inflating: build/bin/llama-save-load-state  \n",
            "  inflating: build/bin/llama-server  \n",
            "  inflating: build/bin/llama-simple  \n",
            "  inflating: build/bin/llama-simple-chat  \n",
            "  inflating: build/bin/llama-speculative  \n",
            "  inflating: build/bin/llama-speculative-simple  \n",
            "  inflating: build/bin/llama-tokenize  \n",
            "  inflating: build/bin/llama-tts     \n",
            "  inflating: build/bin/llama-vdot    \n",
            "  inflating: build/bin/rpc-server    \n",
            "  inflating: build/bin/test-arg-parser  \n",
            "  inflating: build/bin/test-autorelease  \n",
            "  inflating: build/bin/test-backend-ops  \n",
            "  inflating: build/bin/test-barrier  \n",
            "  inflating: build/bin/test-c        \n",
            "  inflating: build/bin/test-chat     \n",
            "  inflating: build/bin/test-chat-template  \n",
            "  inflating: build/bin/test-gguf     \n",
            "  inflating: build/bin/test-grammar-integration  \n",
            "  inflating: build/bin/test-grammar-parser  \n",
            "  inflating: build/bin/test-json-schema-to-grammar  \n",
            "  inflating: build/bin/test-llama-grammar  \n",
            "  inflating: build/bin/test-log      \n",
            "  inflating: build/bin/test-model-load-cancel  \n",
            "  inflating: build/bin/test-quantize-fns  \n",
            "  inflating: build/bin/test-quantize-perf  \n",
            "  inflating: build/bin/test-rope     \n",
            "  inflating: build/bin/test-sampling  \n",
            "  inflating: build/bin/test-tokenizer-0  \n",
            "  inflating: build/bin/test-tokenizer-1-bpe  \n",
            "  inflating: build/bin/test-tokenizer-1-spm  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/content/build/bin/llama-cli -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzGEUgkqX2gN",
        "outputId": "70b4b99c-f609-427d-ae66-ab887b6b0cec"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/build/bin/llama-cli: error while loading shared libraries: libllama.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ggml-org/llama.cpp/releases/download/b4716/llama-b4716-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKvH6yR3X7Am",
        "outputId": "6da1ead3-277e-4cd0-80a9-fa4f2c32ec6a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-10 00:07:10--  https://github.com/ggml-org/llama.cpp/releases/download/b4716/llama-b4716-bin-ubuntu-x64.zip\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/612354784/91152e39-8da8-485d-9685-9e84587a98c2?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250310%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250310T000710Z&X-Amz-Expires=300&X-Amz-Signature=cfa6dcfeb36ca9171a40fc78c624918b259eb66508f66d4db83c134c824b61c9&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllama-b4716-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-10 00:07:10--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/612354784/91152e39-8da8-485d-9685-9e84587a98c2?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250310%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250310T000710Z&X-Amz-Expires=300&X-Amz-Signature=cfa6dcfeb36ca9171a40fc78c624918b259eb66508f66d4db83c134c824b61c9&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllama-b4716-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 31419932 (30M) [application/octet-stream]\n",
            "Saving to: ‘llama-b4716-bin-ubuntu-x64.zip’\n",
            "\n",
            "llama-b4716-bin-ubu 100%[===================>]  29.96M  41.0MB/s    in 0.7s    \n",
            "\n",
            "2025-03-10 00:07:11 (41.0 MB/s) - ‘llama-b4716-bin-ubuntu-x64.zip’ saved [31419932/31419932]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip llama-b4716-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiGuOWjpYH8F",
        "outputId": "b2967a1c-1b5e-43dc-f9a7-52629362905b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  llama-b4716-bin-ubuntu-x64.zip\n",
            "  inflating: build/bin/LICENSE       \n",
            "  inflating: build/bin/LICENSE.linenoise.cpp  \n",
            "  inflating: build/bin/libggml-base.so  \n",
            "  inflating: build/bin/libggml-cpu.so  \n",
            "  inflating: build/bin/libggml-rpc.so  \n",
            "  inflating: build/bin/libggml.so    \n",
            "  inflating: build/bin/libllama.so   \n",
            "  inflating: build/bin/libllava_shared.so  \n",
            "  inflating: build/bin/llama-batched  \n",
            "  inflating: build/bin/llama-batched-bench  \n",
            "  inflating: build/bin/llama-bench   \n",
            "  inflating: build/bin/llama-cli     \n",
            "  inflating: build/bin/llama-convert-llama2c-to-ggml  \n",
            "  inflating: build/bin/llama-cvector-generator  \n",
            "  inflating: build/bin/llama-embedding  \n",
            "  inflating: build/bin/llama-eval-callback  \n",
            "  inflating: build/bin/llama-export-lora  \n",
            "  inflating: build/bin/llama-gbnf-validator  \n",
            "  inflating: build/bin/llama-gen-docs  \n",
            "  inflating: build/bin/llama-gguf    \n",
            "  inflating: build/bin/llama-gguf-hash  \n",
            "  inflating: build/bin/llama-gguf-split  \n",
            "  inflating: build/bin/llama-gritlm  \n",
            "  inflating: build/bin/llama-imatrix  \n",
            "  inflating: build/bin/llama-infill  \n",
            "  inflating: build/bin/llama-llava-cli  \n",
            "  inflating: build/bin/llama-llava-clip-quantize-cli  \n",
            "  inflating: build/bin/llama-lookahead  \n",
            "  inflating: build/bin/llama-lookup  \n",
            "  inflating: build/bin/llama-lookup-create  \n",
            "  inflating: build/bin/llama-lookup-merge  \n",
            "  inflating: build/bin/llama-lookup-stats  \n",
            "  inflating: build/bin/llama-minicpmv-cli  \n",
            "  inflating: build/bin/llama-parallel  \n",
            "  inflating: build/bin/llama-passkey  \n",
            "  inflating: build/bin/llama-perplexity  \n",
            "  inflating: build/bin/llama-q8dot   \n",
            "  inflating: build/bin/llama-quantize  \n",
            "  inflating: build/bin/llama-quantize-stats  \n",
            "  inflating: build/bin/llama-qwen2vl-cli  \n",
            "  inflating: build/bin/llama-retrieval  \n",
            "  inflating: build/bin/llama-run     \n",
            "  inflating: build/bin/llama-save-load-state  \n",
            "  inflating: build/bin/llama-server  \n",
            "  inflating: build/bin/llama-simple  \n",
            "  inflating: build/bin/llama-simple-chat  \n",
            "  inflating: build/bin/llama-speculative  \n",
            "  inflating: build/bin/llama-speculative-simple  \n",
            "  inflating: build/bin/llama-tokenize  \n",
            "  inflating: build/bin/llama-tts     \n",
            "  inflating: build/bin/llama-vdot    \n",
            "  inflating: build/bin/rpc-server    \n",
            "  inflating: build/bin/test-arg-parser  \n",
            "  inflating: build/bin/test-autorelease  \n",
            "  inflating: build/bin/test-backend-ops  \n",
            "  inflating: build/bin/test-barrier  \n",
            "  inflating: build/bin/test-c        \n",
            "  inflating: build/bin/test-chat     \n",
            "  inflating: build/bin/test-chat-template  \n",
            "  inflating: build/bin/test-gguf     \n",
            "  inflating: build/bin/test-grammar-integration  \n",
            "  inflating: build/bin/test-grammar-parser  \n",
            "  inflating: build/bin/test-json-schema-to-grammar  \n",
            "  inflating: build/bin/test-llama-grammar  \n",
            "  inflating: build/bin/test-log      \n",
            "  inflating: build/bin/test-model-load-cancel  \n",
            "  inflating: build/bin/test-quantize-fns  \n",
            "  inflating: build/bin/test-quantize-perf  \n",
            "  inflating: build/bin/test-rope     \n",
            "  inflating: build/bin/test-sampling  \n",
            "  inflating: build/bin/test-tokenizer-0  \n",
            "  inflating: build/bin/test-tokenizer-1-bpe  \n",
            "  inflating: build/bin/test-tokenizer-1-spm  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9VGvqM9vYRX_",
        "outputId": "5a73edc5-7ad4-49a8-c245-f00d4c0ff97c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build/bin/llama-cli: error while loading shared libraries: libllama.so: cannot open shared object file: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ggml-org/llama.cpp/releases/download/b4034/llama-b4034-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKmUoE5OYLrt",
        "outputId": "2c934404-7bc2-4a2b-864d-faa01dae9d8c"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-10 00:10:09--  https://github.com/ggml-org/llama.cpp/releases/download/b4034/llama-b4034-bin-ubuntu-x64.zip\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/612354784/cb1445f2-c976-4b6d-9e98-63d90d9438f1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250310%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250310T001009Z&X-Amz-Expires=300&X-Amz-Signature=fa240d5fde22ad2a246733954903d20fd899138b74c47b1a3fa8a6119a03aabe&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllama-b4034-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-03-10 00:10:09--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/612354784/cb1445f2-c976-4b6d-9e98-63d90d9438f1?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250310%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250310T001009Z&X-Amz-Expires=300&X-Amz-Signature=fa240d5fde22ad2a246733954903d20fd899138b74c47b1a3fa8a6119a03aabe&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dllama-b4034-bin-ubuntu-x64.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 59374362 (57M) [application/octet-stream]\n",
            "Saving to: ‘llama-b4034-bin-ubuntu-x64.zip’\n",
            "\n",
            "llama-b4034-bin-ubu 100%[===================>]  56.62M  33.5MB/s    in 1.7s    \n",
            "\n",
            "2025-03-10 00:10:11 (33.5 MB/s) - ‘llama-b4034-bin-ubuntu-x64.zip’ saved [59374362/59374362]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip llama-b4034-bin-ubuntu-x64.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ-rTkJeYzw9",
        "outputId": "cdeaf32d-2510-4d04-cf78-1f1a1cb2627e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  llama-b4034-bin-ubuntu-x64.zip\n",
            "  inflating: build/bin/LICENSE       \n",
            "  inflating: build/bin/llama-batched  \n",
            "  inflating: build/bin/llama-batched-bench  \n",
            "  inflating: build/bin/llama-bench   \n",
            "  inflating: build/bin/llama-cli     \n",
            "  inflating: build/bin/llama-convert-llama2c-to-ggml  \n",
            "  inflating: build/bin/llama-cvector-generator  \n",
            "  inflating: build/bin/llama-embedding  \n",
            "  inflating: build/bin/llama-eval-callback  \n",
            "  inflating: build/bin/llama-export-lora  \n",
            "  inflating: build/bin/llama-gbnf-validator  \n",
            "  inflating: build/bin/llama-gguf    \n",
            "  inflating: build/bin/llama-gguf-hash  \n",
            "  inflating: build/bin/llama-gguf-split  \n",
            "  inflating: build/bin/llama-gritlm  \n",
            "  inflating: build/bin/llama-imatrix  \n",
            "  inflating: build/bin/llama-infill  \n",
            "  inflating: build/bin/llama-llava-cli  \n",
            "  inflating: build/bin/llama-lookahead  \n",
            "  inflating: build/bin/llama-lookup  \n",
            "  inflating: build/bin/llama-lookup-create  \n",
            "  inflating: build/bin/llama-lookup-merge  \n",
            "  inflating: build/bin/llama-lookup-stats  \n",
            "  inflating: build/bin/llama-minicpmv-cli  \n",
            "  inflating: build/bin/llama-parallel  \n",
            "  inflating: build/bin/llama-passkey  \n",
            "  inflating: build/bin/llama-perplexity  \n",
            "  inflating: build/bin/llama-q8dot   \n",
            "  inflating: build/bin/llama-quantize  \n",
            "  inflating: build/bin/llama-quantize-stats  \n",
            "  inflating: build/bin/llama-retrieval  \n",
            "  inflating: build/bin/llama-save-load-state  \n",
            "  inflating: build/bin/llama-server  \n",
            "  inflating: build/bin/llama-simple  \n",
            "  inflating: build/bin/llama-simple-chat  \n",
            "  inflating: build/bin/llama-speculative  \n",
            "  inflating: build/bin/llama-tokenize  \n",
            "  inflating: build/bin/llama-vdot    \n",
            "  inflating: build/bin/rpc-server    \n",
            "  inflating: build/bin/test-arg-parser  \n",
            "  inflating: build/bin/test-autorelease  \n",
            "  inflating: build/bin/test-backend-ops  \n",
            "  inflating: build/bin/test-barrier  \n",
            "  inflating: build/bin/test-c        \n",
            "  inflating: build/bin/test-chat-template  \n",
            "  inflating: build/bin/test-grad0    \n",
            "  inflating: build/bin/test-grammar-integration  \n",
            "  inflating: build/bin/test-grammar-parser  \n",
            "  inflating: build/bin/test-json-schema-to-grammar  \n",
            "  inflating: build/bin/test-llama-grammar  \n",
            "  inflating: build/bin/test-log      \n",
            "  inflating: build/bin/test-model-load-cancel  \n",
            "  inflating: build/bin/test-quantize-fns  \n",
            "  inflating: build/bin/test-quantize-perf  \n",
            "  inflating: build/bin/test-rope     \n",
            "  inflating: build/bin/test-sampling  \n",
            "  inflating: build/bin/test-tokenizer-0  \n",
            "  inflating: build/bin/test-tokenizer-1-bpe  \n",
            "  inflating: build/bin/test-tokenizer-1-spm  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIpH0lzOY4O9",
        "outputId": "acffbc52-1b04-4061-92c0-8ffed1ac2698"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- common params -----\n",
            "\n",
            "-h,    --help, --usage                  print usage and exit\n",
            "--version                               show version and build info\n",
            "--verbose-prompt                        print a verbose prompt before generation (default: false)\n",
            "-t,    --threads N                      number of threads to use during generation (default: -1)\n",
            "                                        (env: LLAMA_ARG_THREADS)\n",
            "-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:\n",
            "                                        same as --threads)\n",
            "-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range\n",
            "                                        (default: \"\")\n",
            "-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask\n",
            "--cpu-strict <0|1>                      use strict CPU placement (default: 0)\n",
            "--prio N                                set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)\n",
            "-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch\n",
            "                                        (default: same as --cpu-mask)\n",
            "-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch\n",
            "--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)\n",
            "--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime\n",
            "                                        (default: 0)\n",
            "--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)\n",
            "-c,    --ctx-size N                     size of the prompt context (default: 4096, 0 = loaded from model)\n",
            "                                        (env: LLAMA_ARG_CTX_SIZE)\n",
            "-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity, -2 = until\n",
            "                                        context filled)\n",
            "                                        (env: LLAMA_ARG_N_PREDICT)\n",
            "-b,    --batch-size N                   logical maximum batch size (default: 2048)\n",
            "                                        (env: LLAMA_ARG_BATCH)\n",
            "-ub,   --ubatch-size N                  physical maximum batch size (default: 512)\n",
            "                                        (env: LLAMA_ARG_UBATCH)\n",
            "--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =\n",
            "                                        all)\n",
            "-fa,   --flash-attn                     enable Flash Attention (default: disabled)\n",
            "                                        (env: LLAMA_ARG_FLASH_ATTN)\n",
            "-p,    --prompt PROMPT                  prompt to start generation with\n",
            "                                        if -cnv is set, this will be used as system prompt\n",
            "--no-perf                               disable internal libllama performance timings (default: false)\n",
            "                                        (env: LLAMA_ARG_NO_PERF)\n",
            "-f,    --file FNAME                     a file containing the prompt (default: none)\n",
            "-bf,   --binary-file FNAME              binary file containing the prompt (default: none)\n",
            "-e,    --escape                         process escapes sequences (\\n, \\r, \\t, \\', \\\", \\\\) (default: true)\n",
            "--no-escape                             do not process escape sequences\n",
            "--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by\n",
            "                                        the model\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)\n",
            "--rope-scale N                          RoPE context scaling factor, expands context by a factor of N\n",
            "                                        (env: LLAMA_ARG_ROPE_SCALE)\n",
            "--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from\n",
            "                                        model)\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)\n",
            "--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N\n",
            "                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)\n",
            "--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training\n",
            "                                        context size)\n",
            "                                        (env: LLAMA_ARG_YARN_ORIG_CTX)\n",
            "--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full\n",
            "                                        interpolation)\n",
            "                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)\n",
            "--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)\n",
            "--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: 1.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_SLOW)\n",
            "--yarn-beta-fast N                      YaRN: low correction dim or beta (default: 32.0)\n",
            "                                        (env: LLAMA_ARG_YARN_BETA_FAST)\n",
            "-dkvc, --dump-kv-cache                  verbose print of the KV cache\n",
            "-nkvo, --no-kv-offload                  disable KV offload\n",
            "                                        (env: LLAMA_ARG_NO_KV_OFFLOAD)\n",
            "-ctk,  --cache-type-k TYPE              KV cache data type for K (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_K)\n",
            "-ctv,  --cache-type-v TYPE              KV cache data type for V (default: f16)\n",
            "                                        (env: LLAMA_ARG_CACHE_TYPE_V)\n",
            "-dt,   --defrag-thold N                 KV cache defragmentation threshold (default: -1.0, < 0 - disabled)\n",
            "                                        (env: LLAMA_ARG_DEFRAG_THOLD)\n",
            "-np,   --parallel N                     number of parallel sequences to decode (default: 1)\n",
            "                                        (env: LLAMA_ARG_N_PARALLEL)\n",
            "--rpc SERVERS                           comma separated list of RPC servers\n",
            "                                        (env: LLAMA_ARG_RPC)\n",
            "--mlock                                 force system to keep model in RAM rather than swapping or compressing\n",
            "                                        (env: LLAMA_ARG_MLOCK)\n",
            "--no-mmap                               do not memory-map model (slower load but may reduce pageouts if not\n",
            "                                        using mlock)\n",
            "                                        (env: LLAMA_ARG_NO_MMAP)\n",
            "--numa TYPE                             attempt optimizations that help on some NUMA systems\n",
            "                                        - distribute: spread execution evenly over all nodes\n",
            "                                        - isolate: only spawn threads on CPUs on the node that execution\n",
            "                                        started on\n",
            "                                        - numactl: use the CPU map provided by numactl\n",
            "                                        if run without this previously, it is recommended to drop the system\n",
            "                                        page cache before using this\n",
            "                                        see https://github.com/ggerganov/llama.cpp/issues/1437\n",
            "                                        (env: LLAMA_ARG_NUMA)\n",
            "-ngl,  --gpu-layers, --n-gpu-layers N   number of layers to store in VRAM\n",
            "                                        (env: LLAMA_ARG_N_GPU_LAYERS)\n",
            "-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:\n",
            "                                        - none: use one GPU only\n",
            "                                        - layer (default): split layers and KV across GPUs\n",
            "                                        - row: split rows across GPUs\n",
            "                                        (env: LLAMA_ARG_SPLIT_MODE)\n",
            "-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of\n",
            "                                        proportions, e.g. 3,1\n",
            "                                        (env: LLAMA_ARG_TENSOR_SPLIT)\n",
            "-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for\n",
            "                                        intermediate results and KV (with split-mode = row) (default: 0)\n",
            "                                        (env: LLAMA_ARG_MAIN_GPU)\n",
            "--check-tensors                         check model tensor data for invalid values (default: false)\n",
            "--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified\n",
            "                                        multiple times.\n",
            "                                        types: int, float, bool, str. example: --override-kv\n",
            "                                        tokenizer.ggml.add_bos_token=bool:false\n",
            "--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)\n",
            "--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use\n",
            "                                        multiple adapters)\n",
            "--control-vector FNAME                  add a control vector\n",
            "                                        note: this argument can be repeated to add multiple control vectors\n",
            "--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE\n",
            "                                        note: this argument can be repeated to add multiple scaled control\n",
            "                                        vectors\n",
            "--control-vector-layer-range START END\n",
            "                                        layer range to apply the control vector(s) to, start and end inclusive\n",
            "-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`\n",
            "                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)\n",
            "                                        (env: LLAMA_ARG_MODEL)\n",
            "-mu,   --model-url MODEL_URL            model download url (default: unused)\n",
            "                                        (env: LLAMA_ARG_MODEL_URL)\n",
            "-hfr,  --hf-repo REPO                   Hugging Face model repository (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_REPO)\n",
            "-hff,  --hf-file FILE                   Hugging Face model file (default: unused)\n",
            "                                        (env: LLAMA_ARG_HF_FILE)\n",
            "-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment\n",
            "                                        variable)\n",
            "                                        (env: HF_TOKEN)\n",
            "-ld,   --logdir LOGDIR                  path under which to save YAML logs (no logging if unset)\n",
            "--log-disable                           Log disable\n",
            "--log-file FNAME                        Log to file\n",
            "--log-colors                            Enable colored logging\n",
            "                                        (env: LLAMA_LOG_COLORS)\n",
            "-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for\n",
            "                                        debugging)\n",
            "-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be\n",
            "                                        ignored.\n",
            "                                        (env: LLAMA_LOG_VERBOSITY)\n",
            "--log-prefix                            Enable prefx in log messages\n",
            "                                        (env: LLAMA_LOG_PREFIX)\n",
            "--log-timestamps                        Enable timestamps in log messages\n",
            "                                        (env: LLAMA_LOG_TIMESTAMPS)\n",
            "\n",
            "\n",
            "----- sampling params -----\n",
            "\n",
            "--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by\n",
            "                                        ';'\n",
            "                                        (default: dry;top_k;typ_p;top_p;min_p;xtc;temperature)\n",
            "-s,    --seed SEED                      RNG seed (default: -1, use random seed for -1)\n",
            "--sampling-seq SEQUENCE                 simplified sequence for samplers that will be used (default: dkypmxt)\n",
            "--ignore-eos                            ignore end of stream token and continue generating (implies\n",
            "                                        --logit-bias EOS-inf)\n",
            "--penalize-nl                           penalize newline tokens (default: false)\n",
            "--temp N                                temperature (default: 0.8)\n",
            "--top-k N                               top-k sampling (default: 40, 0 = disabled)\n",
            "--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)\n",
            "--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)\n",
            "--xtc-probability N                     xtc probability (default: 0.0, 0.0 = disabled)\n",
            "--xtc-threshold N                       xtc threshold (default: 0.1, 1.0 = disabled)\n",
            "--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)\n",
            "--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1\n",
            "                                        = ctx_size)\n",
            "--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)\n",
            "--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)\n",
            "--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)\n",
            "--dry-multiplier N                      set DRY sampling multiplier (default: 0.0, 0.0 = disabled)\n",
            "--dry-base N                            set DRY sampling base value (default: 1.75)\n",
            "--dry-allowed-length N                  set allowed length for DRY sampling (default: 2)\n",
            "--dry-penalty-last-n N                  set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 =\n",
            "                                        context size)\n",
            "--dry-sequence-breaker STRING           add sequence breaker for DRY sampling, clearing out default breakers\n",
            "                                        ('\\n', ':', '\"', '*') in the process; use \"none\" to not use any\n",
            "                                        sequence breakers\n",
            "--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)\n",
            "--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)\n",
            "--mirostat N                            use Mirostat sampling.\n",
            "                                        Top K, Nucleus and Locally Typical samplers are ignored if used.\n",
            "                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)\n",
            "--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)\n",
            "--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)\n",
            "-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,\n",
            "                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',\n",
            "                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'\n",
            "--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/\n",
            "                                        dir) (default: '')\n",
            "--grammar-file FNAME                    file to read grammar from\n",
            "-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.\n",
            "                                        `{}` for any JSON object\n",
            "                                        For schemas w/ external $refs, use --grammar +\n",
            "                                        example/json_schema_to_grammar.py instead\n",
            "\n",
            "\n",
            "----- example-specific params -----\n",
            "\n",
            "--no-display-prompt                     don't print prompt at generation (default: false)\n",
            "-co,   --color                          colorise output to distinguish prompt and user input from generations\n",
            "                                        (default: false)\n",
            "--no-context-shift                      disables context shift on inifinite text generation (default:\n",
            "                                        disabled)\n",
            "                                        (env: LLAMA_ARG_NO_CONTEXT_SHIFT)\n",
            "-ptc,  --print-token-count N            print token count every N tokens (default: -1)\n",
            "--prompt-cache FNAME                    file to cache prompt state for faster startup (default: none)\n",
            "--prompt-cache-all                      if specified, saves user input and generations to cache as well\n",
            "--prompt-cache-ro                       if specified, uses the prompt cache but does not update it\n",
            "-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode\n",
            "-sp,   --special                        special tokens output enabled (default: false)\n",
            "-cnv,  --conversation                   run in conversation mode:\n",
            "                                        - does not print special tokens and suffix/prefix\n",
            "                                        - interactive mode is also enabled\n",
            "                                        (default: false)\n",
            "-i,    --interactive                    run in interactive mode (default: false)\n",
            "-if,   --interactive-first              run in interactive mode and wait for input right away (default: false)\n",
            "-mli,  --multiline-input                allows you to write or paste multiple lines without ending each in '\\'\n",
            "--in-prefix-bos                         prefix BOS to user inputs, preceding the `--in-prefix` string\n",
            "--in-prefix STRING                      string to prefix user inputs with (default: empty)\n",
            "--in-suffix STRING                      string to suffix after user inputs with (default: empty)\n",
            "--no-warmup                             skip warming up the model with an empty run\n",
            "-gan,  --grp-attn-n N                   group-attention factor (default: 1)\n",
            "                                        (env: LLAMA_ARG_GRP_ATTN_N)\n",
            "-gaw,  --grp-attn-w N                   group-attention width (default: 512)\n",
            "                                        (env: LLAMA_ARG_GRP_ATTN_W)\n",
            "--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's\n",
            "                                        metadata)\n",
            "                                        if suffix/prefix are specified, template will be disabled\n",
            "                                        only commonly used templates are accepted:\n",
            "                                        https://github.com/ggerganov/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template\n",
            "                                        (env: LLAMA_ARG_CHAT_TEMPLATE)\n",
            "--simple-io                             use basic IO for better compatibility in subprocesses and limited\n",
            "                                        consoles\n",
            "\n",
            "example usage:\n",
            "\n",
            "  text generation:     build/bin/llama-cli -m your_model.gguf -p \"I believe the meaning of life is\" -n 128\n",
            "\n",
            "  chat (conversation): build/bin/llama-cli -m your_model.gguf -p \"You are a helpful assistant\" -cnv\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -m /content/Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf -p \"hi\" -n 10 --temp 0.3 --color"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpiG5SrMZNV-",
        "outputId": "56cd080f-ba87-47bc-b0f3-ec97e774a75a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 4034 (b8deef0e) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 256 tensors from /content/Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Meta Llama\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Llama-3.2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Llama-3.2-3B-Instruct-unc...\n",
            "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 196\n",
            "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type q4_K:  169 tensors\n",
            "llama_model_loader: - type q6_K:   29 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_layer          = 28\n",
            "llm_load_print_meta: n_head           = 24\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 3\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 3.61 B\n",
            "llm_load_print_meta: model size       = 2.08 GiB (4.95 BPW) \n",
            "llm_load_print_meta: general.name     = Llama 3.2 3B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: offloading 0 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 0/29 layers to GPU\n",
            "llm_load_tensors: CPU_Mapped model buffer size =  2129.71 MiB\n",
            "..............................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 4096\n",
            "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init:        CPU KV buffer size =   448.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  448.00 MiB, K (f16):  224.00 MiB, V (f16):  224.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   256.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 902\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "\n",
            "sampler seed: 781017736\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.300\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = 10, n_keep = 1\n",
            "\n",
            "\u001b[33mhi\u001b[0m, I'm new to the community and I'm\n",
            "\n",
            "llama_perf_sampler_print:    sampling time =       2.20 ms /    12 runs   (    0.18 ms per token,  5444.65 tokens per second)\n",
            "llama_perf_context_print:        load time =    2377.19 ms\n",
            "llama_perf_context_print: prompt eval time =     753.93 ms /     2 tokens (  376.96 ms per token,     2.65 tokens per second)\n",
            "llama_perf_context_print:        eval time =    3414.38 ms /     9 runs   (  379.38 ms per token,     2.64 tokens per second)\n",
            "llama_perf_context_print:       total time =    4176.28 ms /    11 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -m /content/Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf -p \"hi\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgS0l1_mY5sN",
        "outputId": "64918856-0970-4ce4-fa58-cf5389e1b112"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 4034 (b8deef0e) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 256 tensors from /content/Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Meta Llama\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Llama-3.2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Llama-3.2-3B-Instruct-unc...\n",
            "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 196\n",
            "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type q4_K:  169 tensors\n",
            "llama_model_loader: - type q6_K:   29 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_layer          = 28\n",
            "llm_load_print_meta: n_head           = 24\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 3\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 3.61 B\n",
            "llm_load_print_meta: model size       = 2.08 GiB (4.95 BPW) \n",
            "llm_load_print_meta: general.name     = Llama 3.2 3B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: offloading 0 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 0/29 layers to GPU\n",
            "llm_load_tensors: CPU_Mapped model buffer size =  2129.71 MiB\n",
            "..............................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 4096\n",
            "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init:        CPU KV buffer size =   448.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  448.00 MiB, K (f16):  224.00 MiB, V (f16):  224.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   256.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 902\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "\n",
            "sampler seed: 1497048857\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.800\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = -1, n_keep = 1\n",
            "\n",
            "hi everyone welcome to another video on how to build a simple home gym. in this video series we will be showing you some basic and also some more advanced options for building a home gym. today's topic is going to be on how to build a basic home gym on a budget. so let's get started.\n",
            "\n",
            "**Building a Basic Home Gym on a Budget**\n",
            "\n",
            "building a basic home gym is not as complicated as it sounds. the key is to start with the basics and then add more equipment over time. here are some options to consider:\n",
            "\n",
            "1.  **Pull-up bar:** this is the most essential piece of equipment you'll need to start a basic home gym. you can find pull-up bars at most home goods stores or online. they are usually under 20 dollars and they serve multiple purposes. you can use it for pull-ups, leg raises, and even as a bar for weightlifting.\n",
            "\n",
            "2.  **Resistance bands:** these are great for beginners as they are inexpensive and versatile. you can use them for strength training exercises like bicep curls, tricep extensions, and chest presses.\n",
            "\n",
            "3.  **Kettlebell:** kettlebells are great for strength training and are also inexpensive. they are great for exercises like swings, cleans, and presses.\n",
            "\n",
            "4.  **Exercise mat:** this is essential for any gym as it provides a comfortable surface to exercise on.\n",
            "\n",
            "5.  **Space to exercise:** find a safe and quiet space to exercise in your home.\n",
            "\n",
            "**Tips for Building a Home Gym on a Budget**\n",
            "\n",
            "1.  **Buy used equipment:** you can find used equipment online or at local classifieds. just make sure to inspect the equipment before purchasing.\n",
            "\n",
            "2. "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -m /content/Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf -p \"Write a Python code to print the number 9 ten times. \" -n 55 --temp 0.3 --color"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF4bdpOIZn2l",
        "outputId": "edd82651-2609-447c-9b5c-a25fabeaa940"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 4034 (b8deef0e) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 256 tensors from /content/Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Meta Llama\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Llama-3.2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Llama-3.2-3B-Instruct-unc...\n",
            "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 196\n",
            "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type q4_K:  169 tensors\n",
            "llama_model_loader: - type q6_K:   29 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_layer          = 28\n",
            "llm_load_print_meta: n_head           = 24\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 3\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 3.61 B\n",
            "llm_load_print_meta: model size       = 2.08 GiB (4.95 BPW) \n",
            "llm_load_print_meta: general.name     = Llama 3.2 3B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: offloading 0 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 0/29 layers to GPU\n",
            "llm_load_tensors: CPU_Mapped model buffer size =  2129.71 MiB\n",
            "..............................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 4096\n",
            "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init:        CPU KV buffer size =   448.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  448.00 MiB, K (f16):  224.00 MiB, V (f16):  224.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   256.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 902\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "\n",
            "sampler seed: 3417162502\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.300\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "\u001b[33mgenerate: n_ctx = 4096, n_batch = 2048, n_predict = 55, n_keep = 1\n",
            "\n",
            "Write\u001b[0m a Python code to print the number 9 ten times. 9\n",
            "Here is a simple Python code snippet that prints the number 9 ten times:\n",
            "```\n",
            "# Print the number 9 ten times\n",
            "for i in range(10):\n",
            "    print(9)\n",
            "```\n",
            "This code uses a for loop to iterate 10 times,\n",
            "\n",
            "llama_perf_sampler_print:    sampling time =      10.49 ms /    70 runs   (    0.15 ms per token,  6670.48 tokens per second)\n",
            "llama_perf_context_print:        load time =    1805.05 ms\n",
            "llama_perf_context_print: prompt eval time =    2959.49 ms /    15 tokens (  197.30 ms per token,     5.07 tokens per second)\n",
            "llama_perf_context_print:        eval time =   19015.11 ms /    54 runs   (  352.13 ms per token,     2.84 tokens per second)\n",
            "llama_perf_context_print:       total time =   22010.96 ms /    69 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cHCgIIoZ4E1",
        "outputId": "3b801bae-b3aa-4c9a-9957-6c91002c9720"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "9\n",
            "9\n",
            "9\n",
            "9\n",
            "9\n",
            "9\n",
            "9\n",
            "9\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(2222222)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bz-X5yVPZ4e1",
        "outputId": "5122dd64-465a-42c8-d335-a597c1d0d3a6"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2222222\n",
            "2222222\n",
            "2222222\n",
            "2222222\n",
            "2222222\n",
            "2222222\n",
            "2222222\n",
            "2222222\n",
            "2222222\n",
            "2222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "    print(555555555555555555555555555)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZ4_qZKIZ7lN",
        "outputId": "8eae4690-2ba9-416b-b777-8a61591e7771"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "555555555555555555555555555\n",
            "555555555555555555555555555\n",
            "555555555555555555555555555\n",
            "555555555555555555555555555\n",
            "555555555555555555555555555\n",
            "555555555555555555555555555\n",
            "555555555555555555555555555\n",
            "555555555555555555555555555\n",
            "555555555555555555555555555\n",
            "555555555555555555555555555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!build/bin/llama-cli -m /content/Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf -p \"Who is Napoleon Bonaparte?\" -n 128 --temp 0.3 --color"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBDkt7JGZ-Ml",
        "outputId": "9db421b4-d878-4a74-92f0-ce9ef70252eb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "build: 4034 (b8deef0e) with cc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0 for x86_64-linux-gnu\n",
            "main: llama backend init\n",
            "main: load the model and apply lora adapter, if any\n",
            "llama_model_loader: loaded meta data with 33 key-value pairs and 256 tensors from /content/Llama-3.2-3B-Instruct-uncensored-Q4_K_M.gguf (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = Llama 3.2 3B Instruct\n",
            "llama_model_loader: - kv   3:                       general.organization str              = Meta Llama\n",
            "llama_model_loader: - kv   4:                           general.finetune str              = Instruct\n",
            "llama_model_loader: - kv   5:                           general.basename str              = Llama-3.2\n",
            "llama_model_loader: - kv   6:                         general.size_label str              = 3B\n",
            "llama_model_loader: - kv   7:                          llama.block_count u32              = 28\n",
            "llama_model_loader: - kv   8:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv   9:                     llama.embedding_length u32              = 3072\n",
            "llama_model_loader: - kv  10:                  llama.feed_forward_length u32              = 8192\n",
            "llama_model_loader: - kv  11:                 llama.attention.head_count u32              = 24\n",
            "llama_model_loader: - kv  12:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  13:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  14:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  15:                 llama.attention.key_length u32              = 128\n",
            "llama_model_loader: - kv  16:               llama.attention.value_length u32              = 128\n",
            "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
            "llama_model_loader: - kv  18:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  19:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  20:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  21:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  22:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  23:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  24:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  25:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.eos_token_id u32              = 128009\n",
            "llama_model_loader: - kv  27:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
            "llama_model_loader: - kv  28:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  29:                      quantize.imatrix.file str              = /models_out/Llama-3.2-3B-Instruct-unc...\n",
            "llama_model_loader: - kv  30:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
            "llama_model_loader: - kv  31:             quantize.imatrix.entries_count i32              = 196\n",
            "llama_model_loader: - kv  32:              quantize.imatrix.chunks_count i32              = 125\n",
            "llama_model_loader: - type  f32:   58 tensors\n",
            "llama_model_loader: - type q4_K:  169 tensors\n",
            "llama_model_loader: - type q6_K:   29 tensors\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 3072\n",
            "llm_load_print_meta: n_layer          = 28\n",
            "llm_load_print_meta: n_head           = 24\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 3\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 8192\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 3B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 3.61 B\n",
            "llm_load_print_meta: model size       = 2.08 GiB (4.95 BPW) \n",
            "llm_load_print_meta: general.name     = Llama 3.2 3B Instruct\n",
            "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
            "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: offloading 0 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 0/29 layers to GPU\n",
            "llm_load_tensors: CPU_Mapped model buffer size =  2129.71 MiB\n",
            "..............................................................................\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 4096\n",
            "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init:        CPU KV buffer size =   448.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  448.00 MiB, K (f16):  224.00 MiB, V (f16):  224.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n",
            "llama_new_context_with_model:        CPU compute buffer size =   256.50 MiB\n",
            "llama_new_context_with_model: graph nodes  = 902\n",
            "llama_new_context_with_model: graph splits = 1\n",
            "common_init_from_params: warming up the model with an empty run - please wait ... (--no-warmup to disable)\n",
            "main: llama threadpool init, n_threads = 1\n",
            "\n",
            "system_info: n_threads = 1 (n_threads_batch = 1) / 2 | AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | AMX_INT8 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
            "\n",
            "sampler seed: 788868009\n",
            "sampler params: \n",
            "\trepeat_last_n = 64, repeat_penalty = 1.000, frequency_penalty = 0.000, presence_penalty = 0.000\n",
            "\tdry_multiplier = 0.000, dry_base = 1.750, dry_allowed_length = 2, dry_penalty_last_n = -1\n",
            "\ttop_k = 40, top_p = 0.950, min_p = 0.050, xtc_probability = 0.000, xtc_threshold = 0.100, typical_p = 1.000, temp = 0.300\n",
            "\tmirostat = 0, mirostat_lr = 0.100, mirostat_ent = 5.000\n",
            "sampler chain: logits -> logit-bias -> penalties -> dry -> top-k -> typical -> top-p -> min-p -> xtc -> temp-ext -> dist \n",
            "generate: n_ctx = 4096, n_batch = 2048, n_predict = 128, n_keep = 1\n",
            "\n",
            "\u001b[33mWho is Napoleon Bonaparte?\u001b[0m (Biography)\n",
            "Napoleon Bonaparte was a French military leader and statesman who rose to prominence during the French Revolution and became Emperor of France. His life and legacy are shrouded in controversy, and his impact on European history is still debated among historians and scholars today.\n",
            "Early Life and Career\n",
            "Napoleon was born on August 15, 1769, in Ajaccio, Corsica, to a family of minor nobility. His early life was marked by a tumultuous relationship with his family, particularly his father, Carlo Buonaparte, who was a lawyer and a politician. Napoleon's mother\n",
            "\n",
            "llama_perf_sampler_print:    sampling time =      23.63 ms /   136 runs   (    0.17 ms per token,  5755.40 tokens per second)\n",
            "llama_perf_context_print:        load time =    4099.52 ms\n",
            "llama_perf_context_print: prompt eval time =    2095.11 ms /     8 tokens (  261.89 ms per token,     3.82 tokens per second)\n",
            "llama_perf_context_print:        eval time =   44747.73 ms /   127 runs   (  352.34 ms per token,     2.84 tokens per second)\n",
            "llama_perf_context_print:       total time =   46923.83 ms /   135 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "/content/Llama-3.2-3B-Instruct-uncensored-Q4_K_M_gguf"
      ],
      "metadata": {
        "id": "tS0mEMi6bKbN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}